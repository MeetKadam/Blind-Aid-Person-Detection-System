{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QV-7ZOsxiz1b",
        "outputId": "6f17f3d1-d95a-4ff0-d536-d518ea7de8c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "# 1. Install necessary libraries\n",
        "!pip install torch torchvision numpy Pillow\n",
        "\n",
        "# 2. Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unzip Dataset"
      ],
      "metadata": {
        "id": "d754KhylnygE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip the file. The -d . extracts contents directly into the current directory (/content/)\n",
        "!unzip -q dataset.zip -d ."
      ],
      "metadata": {
        "id": "-C5F79h5lg0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Intall and Import Libraries"
      ],
      "metadata": {
        "id": "X6VMeCGLn3Vc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install torch torchvision numpy Pillow\n",
        "\n",
        "# Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "import time\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n66pUxW3nnw5",
        "outputId": "6c747502-dd02-4e2c-8c41-9736e9ba7fb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create utils.py (Data & IoU Logic)"
      ],
      "metadata": {
        "id": "lZT5nSW1oA0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile utils.py\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# --- UTILITY: Intersection over Union (IoU) ---\n",
        "def intersection_over_union(box1, box2, box_format=\"midpoint\"):\n",
        "    \"\"\" Calculates IoU for bounding boxes. \"\"\"\n",
        "    # Convert [x_c, y_c, w, h] to [x1, y1, x2, y2]\n",
        "    if box_format == \"midpoint\":\n",
        "        box1_x1 = box1[..., 0:1] - box1[..., 2:3] / 2\n",
        "        box1_y1 = box1[..., 1:2] - box1[..., 3:4] / 2\n",
        "        box1_x2 = box1[..., 0:1] + box1[..., 2:3] / 2\n",
        "        box1_y2 = box1[..., 1:2] + box1[..., 3:4] / 2\n",
        "\n",
        "        box2_x1 = box2[..., 0:1] - box2[..., 2:3] / 2\n",
        "        box2_y1 = box2[..., 1:2] - box2[..., 3:4] / 2\n",
        "        box2_x2 = box2[..., 0:1] + box2[..., 2:3] / 2\n",
        "        box2_y2 = box2[..., 1:2] + box2[..., 3:4] / 2\n",
        "\n",
        "    # Find coordinates of intersection area\n",
        "    x1 = torch.max(box1_x1, box2_x1)\n",
        "    y1 = torch.max(box1_y1, box2_y1)\n",
        "    x2 = torch.min(box1_x2, box2_x2)\n",
        "    y2 = torch.min(box1_y2, box2_y2)\n",
        "\n",
        "    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
        "\n",
        "    # Union area\n",
        "    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
        "    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
        "    union = box1_area + box2_area - intersection + 1e-6 # Add epsilon for stability\n",
        "\n",
        "    return intersection / union\n",
        "\n",
        "# --- CUSTOM DATASET CLASS (Converts YOLO labels to Grid Tensor) ---\n",
        "class CustomYOLODataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_path, S=7, B=1, C=2, transform=None):\n",
        "        self.image_dir = os.path.join(data_path, 'images')\n",
        "        self.label_dir = os.path.join(data_path, 'labels')\n",
        "        self.transform = transform\n",
        "\n",
        "        self.image_files = [f for f in os.listdir(self.image_dir) if f.endswith('.jpg')]\n",
        "\n",
        "        self.S = S  # Grid Size\n",
        "        self.B = B  # Boxes per cell\n",
        "        self.C = C  # Classes (e.g., person, car)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_files[idx]\n",
        "        label_name = img_name.replace('.jpg', '.txt')\n",
        "\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "        label_path = os.path.join(self.label_dir, label_name)\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        boxes = []\n",
        "        if os.path.exists(label_path):\n",
        "            with open(label_path, 'r') as f:\n",
        "                for line in f.readlines():\n",
        "                    # Class x_c y_c w h are normalized to 0-1\n",
        "                    class_id, x_c, y_c, w, h = map(float, line.strip().split())\n",
        "                    boxes.append([class_id, x_c, y_c, w, h])\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # KEY: Convert normalized boxes to the 7x7 Grid Target Tensor\n",
        "        target_tensor = torch.zeros((self.S, self.S, self.C + 5 * self.B))\n",
        "\n",
        "        for box in boxes:\n",
        "            class_id, x_c, y_c, w, h = box\n",
        "            class_id = int(class_id)\n",
        "\n",
        "            # i, j are the grid cell coordinates (row, column)\n",
        "            i = int(self.S * y_c)\n",
        "            j = int(self.S * x_c)\n",
        "\n",
        "            i = min(i, self.S - 1)\n",
        "            j = min(j, self.S - 1)\n",
        "\n",
        "            # x_cell, y_cell are coords relative to the cell (0 to 1)\n",
        "            x_cell = self.S * x_c - j\n",
        "            y_cell = self.S * y_c - i\n",
        "\n",
        "            if target_tensor[i, j, 4] == 0:\n",
        "                target_tensor[i, j, 4] = 1.0 # Set confidence\n",
        "                target_tensor[i, j, 0:4] = torch.tensor([x_cell, y_cell, w, h])\n",
        "                target_tensor[i, j, 5 + class_id] = 1.0 # One-hot class encoding\n",
        "\n",
        "        return image, target_tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_zumxfvnpAb",
        "outputId": "e856c104-57e5-423c-ab47-2b633b82b0a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create model.py (Architecture)"
      ],
      "metadata": {
        "id": "Kf5N5B65oOtx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile model.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class BasicYOLODetector(nn.Module):\n",
        "    def __init__(self, in_channels=3, S=7, B=1, C=2):\n",
        "        super(BasicYOLODetector, self).__init__()\n",
        "        self.S = S\n",
        "        self.B = B\n",
        "        self.C = C\n",
        "\n",
        "        # --- Simplified CNN Backbone ---\n",
        "        self.backbone = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2), # Output feature map will be (256, S, S)\n",
        "        )\n",
        "\n",
        "        # --- Detection Head ---\n",
        "        # Predicts (S*S) * (C + 5*B) outputs\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256 * self.S * self.S, 4096),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Linear(4096, self.S * self.S * (self.C + 5 * self.B))\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)\n",
        "        x = self.head(x)\n",
        "\n",
        "        # Reshape to final grid format: (Batch, S, S, C + 5*B)\n",
        "        return x.reshape(-1, self.S, self.S, self.C + 5 * self.B)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcBxaKRdoP6q",
        "outputId": "cb9d0a0b-8ed7-4cb4-ad02-077301644afd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create train.py (Loss & Training Loop)\n"
      ],
      "metadata": {
        "id": "zs6CijGuoUka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from time import time as t\n",
        "\n",
        "# Import modules created above\n",
        "from model import BasicYOLODetector\n",
        "from utils import CustomYOLODataset, intersection_over_union\n",
        "\n",
        "# --- HYPERPARAMETERS ---\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "LEARNING_RATE = 1e-4\n",
        "BATCH_SIZE = 8\n",
        "WEIGHT_DECAY = 5e-4\n",
        "EPOCHS = 50\n",
        "NUM_CLASSES = 2  # Adjust if you have more classes (e.g., 3 for person, car, dog)\n",
        "GRID_SIZE_S = 7\n",
        "BOXES_PER_CELL_B = 1\n",
        "IMG_SIZE = 448\n",
        "# --- DATA PATH ---\n",
        "TRAIN_DATA_PATH = \"./train\" # Points to the unzipped folder in /content/\n",
        "\n",
        "\n",
        "# --- LOSS FUNCTION ---\n",
        "class YoloLoss(nn.Module):\n",
        "    def __init__(self, S=7, B=1, C=2):\n",
        "        super(YoloLoss, self).__init__()\n",
        "        self.mse = nn.MSELoss(reduction=\"sum\")\n",
        "        self.S = S\n",
        "        self.B = B\n",
        "        self.C = C\n",
        "        self.lambda_coord = 5.0\n",
        "        self.lambda_noobj = 0.5\n",
        "\n",
        "    def forward(self, predictions, target):\n",
        "        predictions = predictions.to(DEVICE)\n",
        "        target = target.to(DEVICE)\n",
        "\n",
        "        object_mask = target[..., 4].unsqueeze(-1)\n",
        "\n",
        "        # 1. BOX COORDINATE LOSS (Weighted)\n",
        "        box_targets = object_mask * target[..., 0:4]\n",
        "        box_predictions = object_mask * predictions[..., 0:4]\n",
        "\n",
        "        # Use square root for width and height\n",
        "        box_predictions[..., 2:4] = torch.sign(box_predictions[..., 2:4]) * torch.sqrt(\n",
        "            torch.abs(box_predictions[..., 2:4] + 1e-6)\n",
        "        )\n",
        "        box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])\n",
        "\n",
        "        box_loss = self.mse(box_predictions, box_targets)\n",
        "\n",
        "        # 2. CONFIDENCE LOSS (NO object, Weighted)\n",
        "        no_object_mask = (1 - object_mask)\n",
        "        no_object_prediction = no_object_mask * predictions[..., 4:5]\n",
        "        no_object_target = no_object_mask * target[..., 4:5]\n",
        "        no_object_loss = self.mse(no_object_prediction, no_object_target)\n",
        "\n",
        "        # 3. CONFIDENCE LOSS (WITH object)\n",
        "        object_prediction = object_mask * predictions[..., 4:5]\n",
        "        object_target = object_mask * target[..., 4:5]\n",
        "        object_loss = self.mse(object_prediction, object_target)\n",
        "\n",
        "        # 4. CLASSIFICATION LOSS\n",
        "        class_targets = object_mask * target[..., 5:5+self.C]\n",
        "        class_predictions = object_mask * predictions[..., 5:5+self.C]\n",
        "        class_loss = self.mse(class_predictions, class_targets)\n",
        "\n",
        "        # --- TOTAL LOSS --- (Normalized by batch size)\n",
        "        total_loss = (\n",
        "            self.lambda_coord * box_loss\n",
        "            + object_loss\n",
        "            + self.lambda_noobj * no_object_loss\n",
        "            + class_loss\n",
        "        ) / BATCH_SIZE\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "# --- MAIN TRAINING FUNCTION ---\n",
        "\n",
        "def train_fn(train_loader, model, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for i, (image, target) in enumerate(train_loader):\n",
        "        image = image.to(DEVICE)\n",
        "\n",
        "        predictions = model(image)\n",
        "        loss = loss_fn(predictions, target)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    try:\n",
        "        train_dataset = CustomYOLODataset(\n",
        "            data_path=TRAIN_DATA_PATH,\n",
        "            S=GRID_SIZE_S,\n",
        "            B=BOXES_PER_CELL_B,\n",
        "            C=NUM_CLASSES,\n",
        "            transform=transform\n",
        "        )\n",
        "    except FileNotFoundError:\n",
        "        print(\"\\n--- ERROR: Dataset not found ---\")\n",
        "        print(f\"Please check if the TRAIN_DATA_PATH: {TRAIN_DATA_PATH} is correct and contains 'images' and 'labels' subfolders.\")\n",
        "        exit()\n",
        "\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        dataset=train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    model = BasicYOLODetector(S=GRID_SIZE_S, B=BOXES_PER_CELL_B, C=NUM_CLASSES).to(DEVICE)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "    loss_fn = YoloLoss(S=GRID_SIZE_S, B=BOXES_PER_CELL_B, C=NUM_CLASSES)\n",
        "\n",
        "    print(f\"Starting training on {DEVICE} for {EPOCHS} epochs...\")\n",
        "    start_time = t()\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        epoch_start_time = t()\n",
        "        avg_loss = train_fn(train_loader, model, optimizer, loss_fn)\n",
        "        epoch_time = t() - epoch_start_time\n",
        "        print(f\"--- Epoch {epoch + 1}/{EPOCHS} --- Loss: {avg_loss:.4f} (Time: {epoch_time:.2f}s)\")\n",
        "\n",
        "    total_training_time = t() - start_time\n",
        "    # --- FINAL STEP: SAVE THE TRAINED MODEL WEIGHTS ---\n",
        "    MODEL_SAVE_PATH = \"basic_detector_model.pth\"\n",
        "    torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
        "    print(f\"\\nTraining finished! Total time: {total_training_time:.2f}s\")\n",
        "    print(f\"Model weights saved to: /content/{MODEL_SAVE_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufyfv06xoRoh",
        "outputId": "3d166940-8c95-4e64-a5ff-58eb30bd0cef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "GUlSY9g-osps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysKdYJzuoXir",
        "outputId": "b762a198-ab84-43fa-a050-4b423b9d4eab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training on cuda for 50 epochs...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/train.py\", line 141, in <module>\n",
            "    avg_loss = train_fn(train_loader, model, optimizer, loss_fn)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/train.py\", line 90, in train_fn\n",
            "    predictions = model(image)\n",
            "                  ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/model.py\", line 36, in forward\n",
            "    x = self.head(x)\n",
            "        ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\", line 244, in forward\n",
            "    input = module(input)\n",
            "            ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\", line 125, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: mat1 and mat2 shapes cannot be multiplied (8x200704 and 12544x4096)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fixing Error"
      ],
      "metadata": {
        "id": "-Lht4SlPo9qX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile model.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class BasicYOLODetector(nn.Module):\n",
        "    def __init__(self, in_channels=3, S=7, B=1, C=2):\n",
        "        super(BasicYOLODetector, self).__init__()\n",
        "        self.S = S\n",
        "        self.B = B\n",
        "        self.C = C\n",
        "\n",
        "        # --- Simplified CNN Backbone ---\n",
        "        self.backbone = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2), # Output size: 112x112\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2), # Output size: 56x56\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2), # Output size: 28x28 (Final Feature Map)\n",
        "        )\n",
        "\n",
        "        # --- Detection Head ---\n",
        "\n",
        "        # The input size is 256 (channels) * 28 * 28 (feature map spatial size)\n",
        "        FINAL_FEATURE_SIZE = 256 * 28 * 28 # = 200704\n",
        "        OUTPUT_SIZE = self.S * self.S * (self.C + 5 * self.B)\n",
        "\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(FINAL_FEATURE_SIZE, 4096), # FIXED INPUT SIZE\n",
        "            nn.Dropout(0.5),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Linear(4096, OUTPUT_SIZE)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)\n",
        "        x = self.head(x)\n",
        "\n",
        "        return x.reshape(-1, self.S, self.S, self.C + 5 * self.B)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-dBB3Xyorl5",
        "outputId": "0c744290-a3bc-4618-8c1f-6d582d84227b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rerun"
      ],
      "metadata": {
        "id": "XYUz5Y-TpBx0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrA0W6eso8XS",
        "outputId": "dd36f8cd-bf14-4031-ff34-5bf29e571de4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training on cuda for 50 epochs...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/train.py\", line 141, in <module>\n",
            "    avg_loss = train_fn(train_loader, model, optimizer, loss_fn)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/train.py\", line 95, in train_fn\n",
            "    optimizer.step()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\", line 516, in wrapper\n",
            "    out = func(*args, **kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\", line 81, in _use_grad\n",
            "    ret = func(*args, **kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/optim/adam.py\", line 247, in step\n",
            "    adam(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\", line 149, in maybe_fallback\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/optim/adam.py\", line 949, in adam\n",
            "    func(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/optim/adam.py\", line 682, in _multi_tensor_adam\n",
            "    device_grads = torch._foreach_add(  # type: ignore[assignment]\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.06 GiB. GPU 0 has a total capacity of 14.74 GiB of which 2.24 GiB is free. Process 46828 has 12.49 GiB memory in use. Of the allocated memory 12.31 GiB is allocated by PyTorch, and 46.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reducing Batch Size and Image Size"
      ],
      "metadata": {
        "id": "sAk_F1UNpykP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from time import time as t\n",
        "\n",
        "# Import modules created above\n",
        "from model import BasicYOLODetector\n",
        "from utils import CustomYOLODataset, intersection_over_union\n",
        "\n",
        "# --- HYPERPARAMETERS ---\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "LEARNING_RATE = 1e-4\n",
        "BATCH_SIZE = 4\n",
        "WEIGHT_DECAY = 5e-4\n",
        "EPOCHS = 50\n",
        "NUM_CLASSES = 2  # Adjust if you have more classes (e.g., 3 for person, car, dog)\n",
        "GRID_SIZE_S = 7\n",
        "BOXES_PER_CELL_B = 1\n",
        "IMG_SIZE = 224\n",
        "# --- DATA PATH ---\n",
        "TRAIN_DATA_PATH = \"./train\" # Points to the unzipped folder in /content/\n",
        "\n",
        "\n",
        "# --- LOSS FUNCTION ---\n",
        "class YoloLoss(nn.Module):\n",
        "    def __init__(self, S=7, B=1, C=2):\n",
        "        super(YoloLoss, self).__init__()\n",
        "        self.mse = nn.MSELoss(reduction=\"sum\")\n",
        "        self.S = S\n",
        "        self.B = B\n",
        "        self.C = C\n",
        "        self.lambda_coord = 5.0\n",
        "        self.lambda_noobj = 0.5\n",
        "\n",
        "    def forward(self, predictions, target):\n",
        "        predictions = predictions.to(DEVICE)\n",
        "        target = target.to(DEVICE)\n",
        "\n",
        "        object_mask = target[..., 4].unsqueeze(-1)\n",
        "\n",
        "        # 1. BOX COORDINATE LOSS (Weighted)\n",
        "        box_targets = object_mask * target[..., 0:4]\n",
        "        box_predictions = object_mask * predictions[..., 0:4]\n",
        "\n",
        "        # Use square root for width and height\n",
        "        box_predictions[..., 2:4] = torch.sign(box_predictions[..., 2:4]) * torch.sqrt(\n",
        "            torch.abs(box_predictions[..., 2:4] + 1e-6)\n",
        "        )\n",
        "        box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])\n",
        "\n",
        "        box_loss = self.mse(box_predictions, box_targets)\n",
        "\n",
        "        # 2. CONFIDENCE LOSS (NO object, Weighted)\n",
        "        no_object_mask = (1 - object_mask)\n",
        "        no_object_prediction = no_object_mask * predictions[..., 4:5]\n",
        "        no_object_target = no_object_mask * target[..., 4:5]\n",
        "        no_object_loss = self.mse(no_object_prediction, no_object_target)\n",
        "\n",
        "        # 3. CONFIDENCE LOSS (WITH object)\n",
        "        object_prediction = object_mask * predictions[..., 4:5]\n",
        "        object_target = object_mask * target[..., 4:5]\n",
        "        object_loss = self.mse(object_prediction, object_target)\n",
        "\n",
        "        # 4. CLASSIFICATION LOSS\n",
        "        class_targets = object_mask * target[..., 5:5+self.C]\n",
        "        class_predictions = object_mask * predictions[..., 5:5+self.C]\n",
        "        class_loss = self.mse(class_predictions, class_targets)\n",
        "\n",
        "        # --- TOTAL LOSS --- (Normalized by batch size)\n",
        "        total_loss = (\n",
        "            self.lambda_coord * box_loss\n",
        "            + object_loss\n",
        "            + self.lambda_noobj * no_object_loss\n",
        "            + class_loss\n",
        "        ) / BATCH_SIZE\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "# --- MAIN TRAINING FUNCTION ---\n",
        "\n",
        "def train_fn(train_loader, model, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for i, (image, target) in enumerate(train_loader):\n",
        "        image = image.to(DEVICE)\n",
        "\n",
        "        predictions = model(image)\n",
        "        loss = loss_fn(predictions, target)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    try:\n",
        "        train_dataset = CustomYOLODataset(\n",
        "            data_path=TRAIN_DATA_PATH,\n",
        "            S=GRID_SIZE_S,\n",
        "            B=BOXES_PER_CELL_B,\n",
        "            C=NUM_CLASSES,\n",
        "            transform=transform\n",
        "        )\n",
        "    except FileNotFoundError:\n",
        "        print(\"\\n--- ERROR: Dataset not found ---\")\n",
        "        print(f\"Please check if the TRAIN_DATA_PATH: {TRAIN_DATA_PATH} is correct and contains 'images' and 'labels' subfolders.\")\n",
        "        exit()\n",
        "\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        dataset=train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    model = BasicYOLODetector(S=GRID_SIZE_S, B=BOXES_PER_CELL_B, C=NUM_CLASSES).to(DEVICE)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "    loss_fn = YoloLoss(S=GRID_SIZE_S, B=BOXES_PER_CELL_B, C=NUM_CLASSES)\n",
        "\n",
        "    print(f\"Starting training on {DEVICE} for {EPOCHS} epochs...\")\n",
        "    start_time = t()\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        epoch_start_time = t()\n",
        "        avg_loss = train_fn(train_loader, model, optimizer, loss_fn)\n",
        "        epoch_time = t() - epoch_start_time\n",
        "        print(f\"--- Epoch {epoch + 1}/{EPOCHS} --- Loss: {avg_loss:.4f} (Time: {epoch_time:.2f}s)\")\n",
        "\n",
        "    total_training_time = t() - start_time\n",
        "    # --- FINAL STEP: SAVE THE TRAINED MODEL WEIGHTS ---\n",
        "    MODEL_SAVE_PATH = \"basic_detector_model.pth\"\n",
        "    torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
        "    print(f\"\\nTraining finished! Total time: {total_training_time:.2f}s\")\n",
        "    print(f\"Model weights saved to: /content/{MODEL_SAVE_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNzwssd-pBEV",
        "outputId": "2ae9a100-65c1-428b-8adb-48270ec6b132"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpSNNSFGpwCj",
        "outputId": "56344f41-24a1-40e0-cc85-8ae370574b1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training on cuda for 50 epochs...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/train.py\", line 141, in <module>\n",
            "    avg_loss = train_fn(train_loader, model, optimizer, loss_fn)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/train.py\", line 90, in train_fn\n",
            "    predictions = model(image)\n",
            "                  ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/model.py\", line 40, in forward\n",
            "    x = self.head(x)\n",
            "        ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\", line 244, in forward\n",
            "    input = module(input)\n",
            "            ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\", line 125, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: mat1 and mat2 shapes cannot be multiplied (4x50176 and 200704x4096)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fixing dimension mismatch error"
      ],
      "metadata": {
        "id": "aS4XwsgzqPRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile model.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class BasicYOLODetector(nn.Module):\n",
        "    def __init__(self, in_channels=3, S=7, B=1, C=2):\n",
        "        super(BasicYOLODetector, self).__init__()\n",
        "        self.S = S\n",
        "        self.B = B\n",
        "        self.C = C\n",
        "\n",
        "        # --- Simplified CNN Backbone (UNCHANGED LAYERS) ---\n",
        "        self.backbone = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "        )\n",
        "\n",
        "        # --- Detection Head (FIXED INPUT SIZE) ---\n",
        "        # The input size must match the actual output of the backbone: 256 * H * W\n",
        "        # Assuming IMG_SIZE=224 was used, H*W = 14*14\n",
        "        FINAL_FEATURE_SIZE = 256 * 14 * 14 # = 50176 (MATCHES ERROR TRACE)\n",
        "        OUTPUT_SIZE = self.S * self.S * (self.C + 5 * self.B)\n",
        "\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(FINAL_FEATURE_SIZE, 4096), # FIXED: Uses 50176 as input size\n",
        "            nn.Dropout(0.5),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Linear(4096, OUTPUT_SIZE)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)\n",
        "        x = self.head(x)\n",
        "\n",
        "        return x.reshape(-1, self.S, self.S, self.C + 5 * self.B)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBCbdby1p49G",
        "outputId": "e5dde590-71d4-4161-90c0-86aff77a5eb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start Training"
      ],
      "metadata": {
        "id": "6YitSXCyqgf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8vKb_7TqOEY",
        "outputId": "a41ac21b-167a-49b9-db38-f6da8c7f92dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training on cuda for 50 epochs...\n",
            "--- Epoch 1/50 --- Loss: 9.4470 (Time: 22.99s)\n",
            "--- Epoch 2/50 --- Loss: 7.1794 (Time: 22.52s)\n",
            "--- Epoch 3/50 --- Loss: 5.9684 (Time: 22.69s)\n",
            "--- Epoch 4/50 --- Loss: 4.9764 (Time: 22.23s)\n",
            "--- Epoch 5/50 --- Loss: 4.1942 (Time: 22.28s)\n",
            "--- Epoch 6/50 --- Loss: 3.6067 (Time: 22.29s)\n",
            "--- Epoch 7/50 --- Loss: 3.1940 (Time: 22.29s)\n",
            "--- Epoch 8/50 --- Loss: 3.4673 (Time: 22.38s)\n",
            "--- Epoch 9/50 --- Loss: 3.0225 (Time: 22.27s)\n",
            "--- Epoch 10/50 --- Loss: 2.4428 (Time: 22.25s)\n",
            "--- Epoch 11/50 --- Loss: 2.3059 (Time: 22.27s)\n",
            "--- Epoch 12/50 --- Loss: 2.1518 (Time: 22.27s)\n",
            "--- Epoch 13/50 --- Loss: 1.9649 (Time: 22.17s)\n",
            "--- Epoch 14/50 --- Loss: 1.8396 (Time: 22.31s)\n",
            "--- Epoch 15/50 --- Loss: 1.8856 (Time: 22.36s)\n",
            "--- Epoch 16/50 --- Loss: 1.7175 (Time: 22.30s)\n",
            "--- Epoch 17/50 --- Loss: 1.6137 (Time: 22.12s)\n",
            "--- Epoch 18/50 --- Loss: 1.5093 (Time: 22.32s)\n",
            "--- Epoch 19/50 --- Loss: 1.4675 (Time: 22.27s)\n",
            "--- Epoch 20/50 --- Loss: 1.3952 (Time: 22.29s)\n",
            "--- Epoch 21/50 --- Loss: 1.3862 (Time: 22.24s)\n",
            "--- Epoch 22/50 --- Loss: 1.3153 (Time: 22.32s)\n",
            "--- Epoch 23/50 --- Loss: 1.3087 (Time: 22.33s)\n",
            "--- Epoch 24/50 --- Loss: 1.2318 (Time: 22.28s)\n",
            "--- Epoch 25/50 --- Loss: 1.2339 (Time: 22.23s)\n",
            "--- Epoch 26/50 --- Loss: 1.2274 (Time: 22.24s)\n",
            "--- Epoch 27/50 --- Loss: 1.1741 (Time: 22.27s)\n",
            "--- Epoch 28/50 --- Loss: 1.1071 (Time: 22.47s)\n",
            "--- Epoch 29/50 --- Loss: 1.0761 (Time: 22.28s)\n",
            "--- Epoch 30/50 --- Loss: 1.0154 (Time: 22.21s)\n",
            "--- Epoch 31/50 --- Loss: 0.9939 (Time: 22.41s)\n",
            "--- Epoch 32/50 --- Loss: 0.9945 (Time: 22.25s)\n",
            "--- Epoch 33/50 --- Loss: 0.9698 (Time: 22.27s)\n",
            "--- Epoch 34/50 --- Loss: 0.9318 (Time: 22.24s)\n",
            "--- Epoch 35/50 --- Loss: 0.9374 (Time: 22.33s)\n",
            "--- Epoch 36/50 --- Loss: 0.9297 (Time: 22.30s)\n",
            "--- Epoch 37/50 --- Loss: 0.8688 (Time: 22.29s)\n",
            "--- Epoch 38/50 --- Loss: 0.9285 (Time: 22.28s)\n",
            "--- Epoch 39/50 --- Loss: 0.8333 (Time: 22.19s)\n",
            "--- Epoch 40/50 --- Loss: 0.8787 (Time: 22.29s)\n",
            "--- Epoch 41/50 --- Loss: 0.8094 (Time: 22.31s)\n",
            "--- Epoch 42/50 --- Loss: 0.8466 (Time: 22.40s)\n",
            "--- Epoch 43/50 --- Loss: 0.8099 (Time: 22.21s)\n",
            "--- Epoch 44/50 --- Loss: 0.7578 (Time: 22.32s)\n",
            "--- Epoch 45/50 --- Loss: 0.7863 (Time: 22.26s)\n",
            "--- Epoch 46/50 --- Loss: 0.7516 (Time: 22.25s)\n",
            "--- Epoch 47/50 --- Loss: 0.7562 (Time: 22.22s)\n",
            "--- Epoch 48/50 --- Loss: 0.7612 (Time: 22.27s)\n",
            "--- Epoch 49/50 --- Loss: 0.8049 (Time: 22.32s)\n",
            "--- Epoch 50/50 --- Loss: 0.7321 (Time: 22.31s)\n",
            "\n",
            "Training finished! Total time: 1115.45s\n",
            "Model weights saved to: /content/basic_detector_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading the model"
      ],
      "metadata": {
        "id": "DBgLT-owxvIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/basic_detector_model.pth')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "YSkgbGhDqSNw",
        "outputId": "8af2cf3a-6d0d-4352-d479-aca769f7b2d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0d80a51b-4bc0-467c-9dbc-da028365ba2c\", \"basic_detector_model.pth\", 829239765)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RZrELik0x1h6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}